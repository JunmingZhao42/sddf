/* pancake heap

        word
              ┌──────────────────────────┐
           0  │ eth regs addr            │
              ├──────────────────────────┤
              │ rx_queue {               │
           1  │ free buffer ptr    ──────┼────►
           2  │ active buffer ptr  ──────┼────► shared mem with virt_rx
           3  │ queue capacity           │
              │}                         │
              ├──────────────────────────┤
              │ tx queue {               │
           4  │ free buffer ptr    ──────┼────►
           5  │ active buffer ptr  ──────┼────► shared mem with virt_tx
           6  │ queue capacity           │
              │}                         │
              ├──────────────────────────┤
              │ hw_ring tx {             │
           7  │  tail index              │
           8  │  head index              │
           9  │  capacity                │
          10  │  buffer descriptors ptr ─┼────► shared mem with device
              │ }                        │
              ├──────────────────────────┤
              │ hw_ring rx {             │
          11  │  tail index              │
          12  │  head index              │
          13  │  capacity                │
          14  │  buffer descriptors ptr ─┼────► shared mem with device
              │ }                        │
              ├──────────────────────────┤
     15..527  │ rx meta data             │
              ├──────────────────────────┤
   527..1039  │ tx meta data             │
              ├──────────────────────────┤
*/

#define IRQ_CH 0
#define TX_CH  1
#define RX_CH  2

#define RX_QUEUE_ADDR   (@base + 1 * @biw)
#define TX_QUEUE_ADDR   (@base + 4 * @biw)
#define RX_MDATA_ADDR   (@base + 15 * @biw)
#define TX_MDATA_ADDR   (@base + 527 * @biw)

#define RX_MDATA_ADDR_IDX(idx)  (RX_MDATA_ADDR + idx * NET_BUFF_DESC_SIZE)
#define TX_MDATA_ADDR_IDX(idx)  (TX_MDATA_ADDR + idx * NET_BUFF_DESC_SIZE)

#define HW_RING_SIZE    (4 * @biw)
#define HW_RING_RX      (@base + 7 * @biw)
#define HW_RING_TX      (@base + 7 * @biw + HW_RING_SIZE)
#define HW_DESCR_RX     (lds 1 (@base + 10 * @biw))
#define HW_DESCR_TX     (lds 1 (@base + 14 * @biw))

fun hw_queue_empty({1,1,1} ring) {
    var tail = ring.0;
    var head = ring.1;
    return head == tail;
}

fun hw_queue_full({1,1,1} ring) {
    var tail = ring.0;
    var head = ring.1;
    var length = ring.2;
    return  ((tail - head) == length);
}

fun main () {
    rx_provide();
    tx_provide();
    return 0;
}

/*
 * `rx_return()`: Transfer "active buffer" from device to `virt_rx`
 * 1. check if `hw_ring_rx` is not empty; if not then exit
 * 2. get a buffer from `head` slot of `hw_ring_rx`, make sure its status is non-empty; if not then exit
 * 3. increment `hw_ring_rx` head, enqueue the buffer to `rx_active`
 * 4. if `rx_active` requires signalling, cancel the signal and notify `virt_rx` via microkit
 */
fun rx_return() {
    var packets_transferred = false;
    var rx_queue = lds {1,1,1} RX_QUEUE_ADDR;
    var rx_active = rx_queue.1;
    var capacity = rx_queue.2;

    while (true) {
        var hw_ring_rx = lds {1,1,1} HW_RING_RX;
        var 1 empty = hw_queue_empty(hw_ring_rx);
        if (empty) {
            break;
        }
        // If buffer slot is still empty, we have processed all packets the device has filled
        var hw_head = hw_ring_rx.1;
        var hw_capacity = hw_ring_rx.2;
        var 1 idx = pnk_modulo(hw_head, hw_capacity);

        // volatile struct descriptor *d = &(hw_ring_rx->descr[hw_ring_rx->head]);
        var dscr_addr = DESCRP_ADDR_IDX(HW_DESCR_RX, idx);
        var descriptor = 0;
        !ld32 descriptor, dscr_addr;
        var 1 not_received = rx_descriptor_empty(descriptor);
        if (not_received) {
            break;
        }

        var mdata_buff = RX_MDATA_ADDR_IDX(idx);
        var 1 len = descriptor_get_len(descriptor);
        st mdata_buff + @biw, len;
        var 1 err = net_enqueue(rx_active, capacity, mdata_buff);

        packets_transferred = true;
        hw_head = hw_head + 1;
        var hw_tail = hw_ring_rx.0;
        st HW_RING_RX, <hw_tail, hw_head>;
    }

    var 1 signal = net_require_signal(rx_active);
    if (packets_transferred && signal) {
        net_cancel_signal(rx_active);
        microkit_notify(RX_CH)
    }
    return 0;
}

/*
 * `rx_provide()`: Transfer "free buffer" from `virt_rx` to device
 * 1. check if `hw_ring_rx` is not full, and `rx_queue` is not empty;  if not then exit
 * 2. dequeue a buffer from `rx_free`, update the buffer stat
 * 3. update the tail slot of `hw_ring_rx` to this buffer, increment `hw_ring_rx` tail
 * 4. set device `rdar` register
 * 5. set `rx_free` as require signalling if `hw_ring_rx` is not full
 * 6. recheck (1), process more if needed
*/
fun rx_provide() {
    var reprocess = true;
    var rx_queue = lds {1,1,1} RX_QUEUE_ADDR;
    var rx_free = rx_queue.0;
    var capacity = rx_queue.2;
    var eth = ETH_REGS;

    while (reprocess) {
        while (true) {
            var hw_ring_rx = lds {1,1,1} HW_RING_RX;
            var 1 empty = net_queue_empty(rx_free);
            var 1 full = hw_queue_full(hw_ring_rx);
            if (full || empty) {
                break;
            }

            var {1,1} net_buffer = net_dequeue(rx_free, capacity);

            var hw_tail = hw_ring_rx.0;
            var hw_capacity = hw_ring_rx.2;
            var 1 idx = pnk_modulo(hw_tail, hw_capacity);

            var mdata_buff = RX_MDATA_ADDR_IDX(idx);
            st mdata_buff, net_buffer;
            rx_update_device_ring_slot(HW_DESCR_RX, idx, net_buffer, hw_capacity);

            hw_tail = hw_tail + 1;
            var hw_head = hw_ring_rx.1;
            st HW_RING_RX, <hw_tail, hw_head>;
            set_device_RDAR();
        }

        // Only request a notification from virtualiser if HW ring not full
        var hw_ring_rx = lds {1,1,1} HW_RING_RX;
        var 1 full = hw_queue_full(hw_ring_rx);
        if (!full) {
            net_request_signal(rx_free);
        } else {
            net_cancel_signal(rx_free);
        }

        reprocess = false;

        var 1 empty = net_queue_empty(rx_free);
        if ((!empty) && (!full)) {
            net_cancel_signal(rx_free);
            reprocess = true;
        }
    }
    return 0;
}

/*
 * `tx_provide()`: Transfer "active buffer" from `virt_tx` to device
 * 1. check if `hw_ring_tx` is not full, and `tx_active` is not empty; if not then exit
 * 2. dequeue a buffer from `tx_active`, update the buffer stat
 * 3. update the tail slot of `hw_ring_tx` to this buffer, increment `hw_ring_tx` tail
 * 4. set device `tdar` register
 * 5. set `tx_active` as require signalling
 * 6. recheck (1), process more if needed
 */
fun tx_provide() {
    var reprocess = true;
    var tx_queue = lds {1,1,1} TX_QUEUE_ADDR;
    var tx_active = tx_queue.1;
    var capacity = tx_queue.2;
    var eth = ETH_REGS;

    while (reprocess) {
        while (true) {
            var hw_ring_tx = lds {1,1,1} HW_RING_TX;
            var 1 empty = net_queue_empty(tx_active);
            var 1 full = hw_queue_full(hw_ring_tx);
            if (full || empty) {
                break;
            }

            var {1,1} net_buffer = net_dequeue(tx_active, capacity);

            var hw_tail = hw_ring_tx.0;
            var hw_capacity = hw_ring_tx.2;
            var 1 idx = pnk_modulo(hw_tail, hw_capacity);

            var mdata_buff = TX_MDATA_ADDR_IDX(idx);
            st mdata_buff, net_buffer;
            var descr = hw_ring_tx.3;
            tx_update_device_ring_slot(HW_DESCR_TX, idx, net_buffer, hw_capacity);

            hw_tail = hw_tail + 1;
            var hw_head = hw_ring_tx.1;
            st HW_RING_TX, <hw_tail, hw_head>;
            set_device_TDAR();
        }

        net_request_signal(tx_active);
        reprocess = false;

        var hw_ring_tx = lds {1,1,1} HW_RING_TX;
        var 1 full = hw_queue_full(hw_ring_tx);
        var 1 empty = net_queue_empty(tx_active);
        if ((!full) && (!empty)) {
            net_cancel_signal(tx_active);
            reprocess = true;
        }
    }
    return 0;
}

/*
 * `tx_return()`: Transfer "free buffer" from device to `virt_tx`
 * 1. check  if `hw_ring_tx` is not empty; if not then exit
 * 2. get a buffer from `head` slot of `hw_ring_tx`, make sure it's been processed by the device; if not then exit
 * 3. increment `hw_ring_tx` head, reset the buffer length, enqueue the buffer to `tx_free`
 * 4. if `rx_free` requires signalling, cancel the signal and notify `virt_tx` via microkit
 */
fun tx_return() {
    var enqueued = false;
    var tx_queue = lds {1,1,1} TX_QUEUE_ADDR;
    var tx_free = tx_queue.0;
    var capacity = tx_queue.2;

    while (true) {
        var hw_ring_tx = lds {1,1,1} HW_RING_TX;
        var 1 empty = hw_queue_empty(hw_ring_tx);
        if (empty) {
            break;
        }

        // Ensure that this buffer has been sent by the device
        var hw_head = hw_ring_tx.1;
        var hw_capacity = hw_ring_tx.2;
        var 1 idx = pnk_modulo(hw_head, hw_ring_tx.2);
        var dscr_addr = DESCRP_ADDR_IDX(HW_DESCR_TX, idx);
        var descriptor = 0;
        !ld32 descriptor, dscr_addr;
        var 1 not_sent = tx_descriptor_ready(descriptor);
        if (not_sent) {
            break;
        }

        var mdata_buff = TX_MDATA_ADDR_IDX(idx);
        st mdata_buff + @biw, 0;
        var 1 err = net_enqueue(tx_free, capacity, mdata_buff);

        enqueued = true;
        hw_head = hw_head + 1;
        var hw_tail = hw_ring_tx.0;
        st HW_RING_TX, <hw_tail, hw_head>;
    }

    var 1 signal = net_require_signal(tx_free);
    if (enqueued && signal) {
        net_cancel_signal(tx_free);
        microkit_notify(TX_CH)
    }
    return 0;
}

fun handle_irq() {
    var 1 EIR = get_device_EIR();
    clear_device_EIR();

    while (true) {
        var 1 rx_work = is_EIR_RXF(EIR);
        var 1 tx_work = is_EIR_TXF(EIR);
        if (rx_work) {
            rx_return();
            rx_provide();
        }
        if (tx_work) {
            tx_return();
            tx_provide();
        }
        if ((!rx_work) && (!tx_work)) {
            break;
        }
        EIR = get_device_EIR();
        clear_device_EIR();
    }
    return 0;
}

export fun notified(1 channel) {
    if (channel == IRQ_CH) {
        handle_irq();
        /*
         * Delay calling into the kernel to ack the IRQ until the next loop
         * in the microkit event handler loop.
         */
        microkit_deferred_irq_ack(channel)
        return 0;
    }

    if (channel == RX_CH) {
        rx_provide();
        return 0;
    }

    if (channel == TX_CH) {
        tx_provide();
        return 0;
    }

    return -1;
}